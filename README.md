
### Installation

**⚠️ Important**: We strongly recommend using [uv](https://docs.astral.sh/uv/) for dependency management instead of pip with `requirements.txt`. The `requirements.txt` file was generated by `uv pip freeze` and contains pinned versions that may not install correctly across different systems.

#### Option 1: Using uv (Recommended)

```bash
# Install uv if you haven't already
curl -LsSf https://astral.sh/uv/install.sh | sh
# or on macOS: brew install uv
# or on Windows: powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"

# Clone the repository
git clone https://github.com/vndee/local-talking-llm.git
cd local-talking-llm

# Install dependencies using uv (recommended)
uv sync

# Activate the virtual environment
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Download NLTK data (for sentence tokenization)
python -c "import nltk; nltk.download('punkt_tab')"
```

#### Option 2: Using pip (Alternative)

If you prefer to use pip, install directly from pyproject.toml:

```bash
# Clone the repository
git clone https://github.com/vndee/local-talking-llm.git
cd local-talking-llm

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install from pyproject.toml (NOT requirements.txt)
pip install -e .

# Download NLTK data
python -c "import nltk; nltk.download('punkt')"
```
#### Install TextToSpeechService with Coqui-ai
```
uv remove chatterbox-tts
uv add coqui-tts


Faster response with : tts_models/en/ljspeech/vits--neon

have to install espeak: brew install espeak
```
#### Install and Setup Ollama

```bash
# Install and start Ollama
# Follow instructions at https://ollama.ai
ollama pull gemma3  # or any other model you prefer
```

### Usage

#### Basic Usage
```bash
python app.py
```

#### With Voice Cloning
Record a 10-30 second audio sample of the voice you want to clone, then:
```bash
python app.py --voice path/to/voice_sample.wav
```

#### With Custom Settings
```bash
# Adjust emotion and pacing
python app.py --exaggeration 0.7 --cfg-weight 0.3

# Use a different LLM model
python app.py --model codellama

# Save generated voice samples
python app.py --save-voice
```

### Configuration Options

- `--voice`: Path to audio file for voice cloning
- `--exaggeration`: Emotion intensity (0.0-1.0, default: 0.5)
  - Lower values (0.3-0.4): Calmer, more neutral delivery
  - Higher values (0.7-0.9): More expressive and emotional
- `--cfg-weight`: Controls pacing and delivery style (0.0-1.0, default: 0.5)
  - Lower values: Faster, more dynamic speech
  - Higher values: Slower, more deliberate speech
- `--model`: Ollama model to use (default: llama2)
- `--save-voice`: Save generated audio responses to `voices/` directory

### Implementation Details




Key improvements over the previous Bark implementation:
- **Voice Cloning**: Pass an audio file to clone any voice
- **Emotion Control**: Adjust expressiveness with the `exaggeration` parameter
- **Better Quality**: ChatterBox produces more natural-sounding speech
- **Faster Inference**: Smaller model size (0.5B vs Bark's larger models)

#### Dynamic Emotion Analysis
The app now includes automatic emotion detection to make responses more expressive:

```python
def analyze_emotion(text: str) -> float:
    emotional_keywords = ['amazing', 'terrible', 'love', 'hate', 'excited',
                         'sad', 'happy', 'angry', '!', '?!']
    emotion_score = 0.5
    for keyword in emotional_keywords:
        if keyword in text.lower():
            emotion_score += 0.1
    return min(0.9, max(0.3, emotion_score))
```

### Tips for Best Results

1. **Voice Cloning**:
   - Use a clear 10-30 second audio sample
   - Ensure the sample has minimal background noise
   - The voice should speak naturally in the sample

2. **Emotion Control**:
   - For general conversation: `exaggeration=0.5, cfg_weight=0.5`
   - For dramatic/expressive speech: `exaggeration=0.7+, cfg_weight=0.3`
   - For calm/professional tone: `exaggeration=0.3, cfg_weight=0.7`

3. **Performance**:
   - Use CUDA if available for faster inference
   - The first generation might be slower due to model loading
   - Consider using smaller Whisper models ("tiny.en" or "base.en") for faster transcription

### Scaling to Production

For those aiming to elevate this application to a production-ready status, consider:

- **Performance Optimization**:
  - Use optimized inference engines (ONNX, TensorRT)
  - Implement model quantization for faster inference
  - Add caching for frequently used phrases

- **Enhanced Features**:
  - Multi-speaker support with voice profiles
  - Real-time voice conversion
  - Integration with more LLM providers
  - Web interface with real-time streaming

- **Voice Database**:
  - Create a library of voice samples
  - Implement voice selection UI
  - Add voice mixing capabilities

- **API Service**:
  - RESTful API for TTS requests
  - WebSocket support for real-time communication
  - Rate limiting and authentication

### Troubleshooting

#### Dependency Installation Issues

**Problem**: `requirements.txt` installation fails with errors like:
- `ERROR: Could not find a version that satisfies the requirement cPython==0.0.6`
- `ModuleNotFoundError: No module named 'distutils'`
- Various version conflicts

**Solution**: The `requirements.txt` file was generated by `uv pip freeze` and contains exact versions that may not work across different systems. Use one of these alternatives:

1. **Use uv (Recommended)**:
   ```bash
   uv sync
   ```

2. **Use pip with pyproject.toml**:
   ```bash
   pip install -e .
   ```

3. **Manual installation of core packages**:
   ```bash
   pip install chatterbox-tts langchain-ollama openai-whisper sounddevice rich nltk
   ```

#### Runtime Issues

- **CUDA out of memory**: Use CPU mode or reduce model precision
- **Microphone not working**: Check system permissions and device settings
- **Slow inference**: Ensure you're using GPU if available, consider using smaller models
- **Voice cloning quality**: Use higher quality audio samples with clear speech
- **Import errors**: Make sure you activated the virtual environment before running the app

### Conclusion

With the integration of ChatterBox, we've significantly enhanced our local voice assistant. The addition of voice cloning and emotion control opens up new possibilities for creating personalized and expressive AI assistants. Whether you're building a personal Jarvis, creating content, or developing voice-enabled applications, this updated stack provides a powerful foundation.

The combination of Whisper's robust speech recognition, Ollama's flexible LLM serving, and ChatterBox's advanced TTS capabilities creates a fully-featured voice assistant that runs entirely offline. No cloud services, no API keys, just pure local AI power!

### Resources

- [ChatterBox GitHub](https://github.com/resemble-ai/chatterbox)
- [Ollama](https://ollama.ai)
- [Whisper](https://github.com/openai/whisper)
- [Original Blog Post](https://blog.duy-huynh.com/build-your-own-voice-assistant-and-run-it-locally/)

---

